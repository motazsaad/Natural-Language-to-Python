{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlv7IfhFQhsv"
      },
      "source": [
        "# Data Perparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNHn32FLc56o",
        "outputId": "a1c4d2f0-279d-45d8-9f90-24526d41c9e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.8.0\n",
            "  Downloading torch-1.8.0-cp37-cp37m-manylinux1_x86_64.whl (735.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 735.5 MB 12 kB/s \n",
            "\u001b[?25hCollecting torchtext==0.9.0\n",
            "  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 40.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0) (4.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2.10)\n",
            "Installing collected packages: torch, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.1+cu113\n",
            "    Uninstalling torch-1.12.1+cu113:\n",
            "      Successfully uninstalled torch-1.12.1+cu113\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.13.1\n",
            "    Uninstalling torchtext-0.13.1:\n",
            "      Successfully uninstalled torchtext-0.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.8.0 which is incompatible.\n",
            "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.8.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.8.0 torchtext-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.8.0 torchtext==0.9.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-mOljzdcIZL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchtext\n",
        "from torchtext.legacy.data import Field, BucketIterator, Iterator\n",
        "from torchtext.legacy import data\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1UTpnAaSUlp"
      },
      "source": [
        "## Reading the text file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXgcDwaTdDdB",
        "outputId": "a7360983-6245-46b6-cc0e-70666febd6de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "input_path = 'drive/My Drive/Colab Notebooks/Master thises/Data Preprocessing/input/input.txt'\n",
        "model_saved_fol_path = 'drive/My Drive/Colab Notebooks/Master thises/Data Modeling/pytorch/06_model/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXl-WZ7mdD0X"
      },
      "outputs": [],
      "source": [
        "dps = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywfGE0HAdLJa"
      },
      "outputs": [],
      "source": [
        "dataset = open(input_path).read().strip().split('EOC\\n')\n",
        "pairs = [line.split(' ||| ') for line in dataset]\n",
        "random.shuffle(pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJT52tLsdMPt"
      },
      "outputs": [],
      "source": [
        "for inp, targ in pairs:\n",
        "  dp = {}\n",
        "  dp['solution'] = targ\n",
        "  dp['question'] = inp\n",
        "  dps.append(dp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qA-1MuOecZRt",
        "outputId": "a6f73f18-e4d7-440c-f823-93fb16cd9d62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset size: 5013\n"
          ]
        }
      ],
      "source": [
        "print(\"Dataset size:\", len(dps))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swBfwcDaXEh5"
      },
      "source": [
        "## Using a custom tokenizer to tokenize python code\n",
        "\n",
        "Python is a programming language with its own unique syntax. Regular tokenizers like spacy are meant to tokenize english scentences and are not optimized towards Python's syntax. Here, we write our own custom tokenizer that makes use of Python's default [tokenize](https://docs.python.org/3/library/tokenize.html) library. When we make use of this library we only extract the token type and the token string.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UsevXhPgh1R"
      },
      "outputs": [],
      "source": [
        "from tokenize import tokenize, untokenize\n",
        "import io\n",
        "\n",
        "\n",
        "def tokenize_python_code(python_code_str):\n",
        "    python_tokens = list(tokenize(io.BytesIO(python_code_str.encode('utf-8')).readline))\n",
        "    tokenized_output = []\n",
        "    for i in range(0, len(python_tokens)):\n",
        "        tokenized_output.append((python_tokens[i].type, python_tokens[i].string))\n",
        "    return tokenized_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2V-sePckv1-K",
        "outputId": "4f7f7cfe-2885-410a-aac4-29ce65e359fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(57, 'utf-8'), (56, '\\n'), (56, '\\n'), (56, '\\n'), (56, '\\n'), (56, '\\n'), (1, 'def'), (1, 'two_sum'), (53, '('), (1, 'nums'), (53, ','), (1, 'target'), (53, ')'), (53, ':'), (4, '\\n'), (5, '  '), (1, 'd'), (53, '='), (53, '{'), (53, '}'), (4, '\\n'), (1, 'for'), (1, 'i'), (53, ','), (1, 'num'), (1, 'in'), (1, 'enumerate'), (53, '('), (1, 'nums'), (53, ')'), (53, ':'), (4, '\\n'), (5, '    '), (1, 'if'), (1, 'target'), (53, '-'), (1, 'num'), (1, 'in'), (1, 'd'), (53, ':'), (4, '\\n'), (5, '        '), (1, 'return'), (53, '['), (1, 'd'), (53, '['), (1, 'target'), (53, '-'), (1, 'num'), (53, ']'), (53, ','), (1, 'i'), (53, ']'), (4, '\\n'), (6, ''), (1, 'else'), (53, ':'), (4, '\\n'), (5, '        '), (1, 'd'), (53, '['), (1, 'num'), (53, ']'), (53, '='), (1, 'i'), (4, '\\n'), (6, ''), (6, ''), (6, ''), (0, '')]\n"
          ]
        }
      ],
      "source": [
        "tokenized_sample = tokenize_python_code(dps[1]['solution'])\n",
        "print(tokenized_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQ00a5cwwHtL",
        "outputId": "5bbb39e6-b5ec-46bb-9b1e-424acd215d49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "def two_sum (nums ,target ):\n",
            "  d ={}\n",
            "  for i ,num in enumerate (nums ):\n",
            "    if target -num in d :\n",
            "        return [d [target -num ],i ]\n",
            "    else :\n",
            "        d [num ]=i \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(untokenize(tokenized_sample).decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nsoc0eCpb8e"
      },
      "source": [
        "Since we have mere 5000 data points, we make use of data augmentations to increase the size of our dataset. While tokenizing the python code, we mask the names of certain variables randomly(with 'var_1, 'var_2' etc) to ensure that the model that we train does not merly fixate on the way the variables are named and actually tries to understand the inhrent logic and syntax of the python code.\n",
        "\n",
        "But, while randomly picking varibles to mask we avoid keyword literals(*keyword.kwlist*), control structures(as can be seen in below *skip_list*) and object properties. We add all such literals that need to be skipped into the *skip_list*\n",
        "\n",
        "```skip_list = ['range', 'enumerate', 'print', 'ord', 'int', 'float', 'char', 'list', 'dict', 'tuple', 'set', 'len', 'sum', 'min', 'max']```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lM-tqDwhYoVJ",
        "outputId": "f7302a85-c868-4066-c14e-472f2cde62db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['False', 'None', 'True', 'and', 'as', 'assert', 'async', 'await', 'break', 'class', 'continue', 'def', 'del', 'elif', 'else', 'except', 'finally', 'for', 'from', 'global', 'if', 'import', 'in', 'is', 'lambda', 'nonlocal', 'not', 'or', 'pass', 'raise', 'return', 'try', 'while', 'with', 'yield']\n"
          ]
        }
      ],
      "source": [
        "import keyword\n",
        "\n",
        "print(keyword.kwlist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbIMyrYapfwS"
      },
      "outputs": [],
      "source": [
        "def augment_tokenize_python_code(python_code_str, mask_factor=0.3):\n",
        "\n",
        "\n",
        "    var_dict = {} # Dictionary that stores masked variables\n",
        "\n",
        "    # certain reserved words that should not be treated as normal variables and\n",
        "    # hence need to be skipped from our variable mask augmentations\n",
        "    skip_list = ['range', 'enumerate', 'print', 'ord', 'int', 'float', 'zip'\n",
        "                 'char', 'list', 'dict', 'tuple', 'set', 'len', 'sum', 'min', 'max']\n",
        "    skip_list.extend(keyword.kwlist)\n",
        "\n",
        "    var_counter = 1\n",
        "    python_tokens = list(tokenize(io.BytesIO(python_code_str.encode('utf-8')).readline))\n",
        "    tokenized_output = []\n",
        "\n",
        "    for i in range(0, len(python_tokens)):\n",
        "      if python_tokens[i].type == 1 and python_tokens[i].string not in skip_list:\n",
        "        \n",
        "        if i>0 and python_tokens[i-1].string in ['def', '.', 'import', 'raise', 'except', 'class']: # avoid masking modules, functions and error literals\n",
        "          skip_list.append(python_tokens[i].string)\n",
        "          tokenized_output.append((python_tokens[i].type, python_tokens[i].string))\n",
        "        elif python_tokens[i].string in var_dict:  # if variable is already masked\n",
        "          tokenized_output.append((python_tokens[i].type, var_dict[python_tokens[i].string]))\n",
        "        elif random.uniform(0, 1) > 1-mask_factor: # randomly mask variables\n",
        "          var_dict[python_tokens[i].string] = 'var_' + str(var_counter)\n",
        "          var_counter+=1\n",
        "          tokenized_output.append((python_tokens[i].type, var_dict[python_tokens[i].string]))\n",
        "        else:\n",
        "          skip_list.append(python_tokens[i].string)\n",
        "          tokenized_output.append((python_tokens[i].type, python_tokens[i].string))\n",
        "      \n",
        "      else:\n",
        "        tokenized_output.append((python_tokens[i].type, python_tokens[i].string))\n",
        "    \n",
        "    return tokenized_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HJwaBJOhdxb",
        "outputId": "31617a5b-42a5-48da-e0c5-d38e15bc688d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(57, 'utf-8'), (56, '\\n'), (56, '\\n'), (56, '\\n'), (56, '\\n'), (56, '\\n'), (1, 'def'), (1, 'two_sum'), (53, '('), (1, 'var_1'), (53, ','), (1, 'target'), (53, ')'), (53, ':'), (4, '\\n'), (5, '  '), (1, 'd'), (53, '='), (53, '{'), (53, '}'), (4, '\\n'), (1, 'for'), (1, 'i'), (53, ','), (1, 'num'), (1, 'in'), (1, 'enumerate'), (53, '('), (1, 'var_1'), (53, ')'), (53, ':'), (4, '\\n'), (5, '    '), (1, 'if'), (1, 'target'), (53, '-'), (1, 'num'), (1, 'in'), (1, 'd'), (53, ':'), (4, '\\n'), (5, '        '), (1, 'return'), (53, '['), (1, 'd'), (53, '['), (1, 'target'), (53, '-'), (1, 'num'), (53, ']'), (53, ','), (1, 'i'), (53, ']'), (4, '\\n'), (6, ''), (1, 'else'), (53, ':'), (4, '\\n'), (5, '        '), (1, 'd'), (53, '['), (1, 'num'), (53, ']'), (53, '='), (1, 'i'), (4, '\\n'), (6, ''), (6, ''), (6, ''), (0, '')]\n"
          ]
        }
      ],
      "source": [
        "tokenized_sample = augment_tokenize_python_code(dps[1]['solution'])\n",
        "print(tokenized_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciLm6oezs3A2",
        "outputId": "f6f70a25-2255-40dd-df8b-f3faded623fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "def two_sum (var_1 ,target ):\n",
            "  d ={}\n",
            "  for i ,num in enumerate (var_1 ):\n",
            "    if target -num in d :\n",
            "        return [d [target -num ],i ]\n",
            "    else :\n",
            "        d [num ]=i \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(untokenize(tokenized_sample).decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hYArh3OznQ0"
      },
      "source": [
        "As one can see our augmented tokenizer picked num2 randomly and masked(replaced) it with by var_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shdWBNLu0DkK"
      },
      "source": [
        "## Building Train and Validation Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhBx_KFVaJIK"
      },
      "outputs": [],
      "source": [
        "python_problems_df = pd.DataFrame(dps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "6i3TKNUoaWPC",
        "outputId": "32fc78a7-4fbc-4559-d7f7-dda1725dced6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-13504d1c-7025-4721-8111-7e5c62ae3ecb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>solution</th>\n",
              "      <th>question</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\\n# Python3 code to demonstrate  \\n# pair iter...</td>\n",
              "      <td>List iteration is common in programming, but s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\\n\\n\\n\\n\\ndef two_sum(nums, target):\\n  d = {}...</td>\n",
              "      <td>Given an array of integers, return indices of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td># Finding Articulation Points in Undirected Gr...</td>\n",
              "      <td>articulation points</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>'\"\"\"\\n\\n\"\"\"\\nfor _ in range(int(input())):\\n  ...</td>\n",
              "      <td>'Problem Link:https://practice.geeksforgeeks.o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\\n# Python3 code to demonstrate working of  \\n...</td>\n",
              "      <td>Sometimes, while working with Python data, we ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-13504d1c-7025-4721-8111-7e5c62ae3ecb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-13504d1c-7025-4721-8111-7e5c62ae3ecb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-13504d1c-7025-4721-8111-7e5c62ae3ecb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                            solution  \\\n",
              "0  \\n# Python3 code to demonstrate  \\n# pair iter...   \n",
              "1  \\n\\n\\n\\n\\ndef two_sum(nums, target):\\n  d = {}...   \n",
              "2  # Finding Articulation Points in Undirected Gr...   \n",
              "3  '\"\"\"\\n\\n\"\"\"\\nfor _ in range(int(input())):\\n  ...   \n",
              "4  \\n# Python3 code to demonstrate working of  \\n...   \n",
              "\n",
              "                                            question  \n",
              "0  List iteration is common in programming, but s...  \n",
              "1  Given an array of integers, return indices of ...  \n",
              "2                                articulation points  \n",
              "3  'Problem Link:https://practice.geeksforgeeks.o...  \n",
              "4  Sometimes, while working with Python data, we ...  "
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "python_problems_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_93zNqfepxY",
        "outputId": "75ce8642-d3a1-4353-9e7c-370f0277fe1d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5013, 2)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "python_problems_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiGB2ffjwYmf"
      },
      "outputs": [],
      "source": [
        "python_problems_df.to_csv(model_saved_fol_path+'python_problems_df.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cdHbscZsSJu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(0)\n",
        "msk = np.random.rand(len(python_problems_df)) < 0.85 # Splitting data into 85% train and 15% validation\n",
        "\n",
        "train_df = python_problems_df[msk]\n",
        "val_df = python_problems_df[~msk]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDtZSkLUu53D",
        "outputId": "392fbd72-6a01-4057-8c12-cab3acd1bd9f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4260, 2)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3SkbK0Qu9TZ",
        "outputId": "8282a3e7-dfb5-4c98-a366-31390db050dc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(753, 2)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57lz1Jfxwq6o"
      },
      "outputs": [],
      "source": [
        "train_df.to_csv(model_saved_fol_path+'train_df.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAV60imswsrH"
      },
      "outputs": [],
      "source": [
        "val_df.to_csv(model_saved_fol_path+'val_df.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u7EyiRZ6brX"
      },
      "source": [
        "## Creating vocabulary using torchtext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3ZQQWsWi4rn"
      },
      "source": [
        "In this section we will use torchtext Fields to construct the vocabulary for our sequence-to-sequence learning problem.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlIi4zsLKwLE"
      },
      "outputs": [],
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLID_2ZtixKY",
        "outputId": "9142d6e9-1f56-4514-ce8a-f9be43b22500"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
            "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
          ]
        }
      ],
      "source": [
        "Input = data.Field(tokenize = 'spacy',\n",
        "            init_token='<sos>', \n",
        "            eos_token='<eos>', \n",
        "            lower=True)\n",
        "\n",
        "Output = data.Field(tokenize = augment_tokenize_python_code,\n",
        "                    init_token='<sos>', \n",
        "                    eos_token='<eos>', \n",
        "                    lower=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttEpbc2rixPx"
      },
      "outputs": [],
      "source": [
        "fields = [('Input', Input),('Output', Output)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxsyNTBtogcD"
      },
      "source": [
        "Since our data augmentations have the potential to increase the vocabulary beyond what it initially is, we must ensure that we capture as many variations as possible in the vocabulary that we develop. In the the below code we apply our data augmentations 100 times to ensure that we can capture a majority of augmentations into our vocabulary. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uP_yJeqjoLLV"
      },
      "outputs": [],
      "source": [
        "train_example = []\n",
        "val_example = []\n",
        "\n",
        "train_expansion_factor = 100\n",
        "for j in range(train_expansion_factor):\n",
        "  for i in range(train_df.shape[0]):\n",
        "      try:\n",
        "          ex = data.Example.fromlist([train_df.question[i], train_df.solution[i]], fields)\n",
        "          train_example.append(ex)\n",
        "      except:\n",
        "          pass\n",
        "\n",
        "for i in range(val_df.shape[0]):\n",
        "    try:\n",
        "        ex = data.Example.fromlist([val_df.question[i], val_df.solution[i]], fields)\n",
        "        val_example.append(ex)\n",
        "    except:\n",
        "        pass       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooT2EqpmtcAW"
      },
      "outputs": [],
      "source": [
        "train_data = data.Dataset(train_example, fields)\n",
        "valid_data =  data.Dataset(val_example, fields)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sY_54gsdygSK",
        "outputId": "bbf9e127-0385-4fc7-c1a7-ab3875b100e1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torchtext.legacy.data.dataset.Dataset at 0x7f0fcdb5bd10>"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uk4zu6Ntqgfr"
      },
      "outputs": [],
      "source": [
        "Input.build_vocab(train_data, min_freq = 0)\n",
        "Output.build_vocab(train_data, min_freq = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oA17CYnDJa4D",
        "outputId": "44990f3e-ef7e-412b-fe1f-cf897c7bf9a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torchtext.vocab.Vocab at 0x7f0fcdb8b110>"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Output.vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5f1HbzYuEcD"
      },
      "outputs": [],
      "source": [
        "def save_vocab(vocab, path):\n",
        "    import pickle\n",
        "    output = open(path, 'wb')\n",
        "    pickle.dump(vocab, output)\n",
        "    output.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lN6tUrOxuFac"
      },
      "outputs": [],
      "source": [
        "save_vocab(Input.vocab, model_saved_fol_path+\"src_vocab.pkl\")\n",
        "save_vocab(Output.vocab, model_saved_fol_path+\"trg_vocab.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pv-5RpJuAjb",
        "outputId": "c748aae6-b8bd-443f-9148-4f531a4eb8cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgNa1xUluGHc",
        "outputId": "a7274d18-d04d-4e22-bd3e-d3b5e5078e6f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(57, 'utf-8'),\n",
              " (56, '\\n'),\n",
              " (55, '# Python3 code to demonstrate\\xa0 '),\n",
              " (56, '\\n'),\n",
              " (55, '# pair iteration in list\\xa0 '),\n",
              " (56, '\\n'),\n",
              " (55, '# using list comprehension '),\n",
              " (56, '\\n'),\n",
              " (1, 'from'),\n",
              " (1, 'var_1'),\n",
              " (1, 'import'),\n",
              " (1, 'compress'),\n",
              " (4, '\\n'),\n",
              " (54, '\\xa0'),\n",
              " (54, '\\xa0'),\n",
              " (4, '\\n'),\n",
              " (55, '# initializing list\\xa0\\xa0 '),\n",
              " (56, '\\n'),\n",
              " (1, 'test_list'),\n",
              " (53, '='),\n",
              " (53, '['),\n",
              " (2, '0'),\n",
              " (53, ','),\n",
              " (2, '1'),\n",
              " (53, ','),\n",
              " (2, '2'),\n",
              " (53, ','),\n",
              " (2, '3'),\n",
              " (53, ','),\n",
              " (2, '4'),\n",
              " (53, ','),\n",
              " (2, '5'),\n",
              " (53, ']'),\n",
              " (4, '\\n'),\n",
              " (54, '\\xa0'),\n",
              " (54, '\\xa0'),\n",
              " (4, '\\n'),\n",
              " (55, '# printing original list '),\n",
              " (56, '\\n'),\n",
              " (1, 'print'),\n",
              " (53, '('),\n",
              " (3, '\"The original list is : \"'),\n",
              " (53, '+'),\n",
              " (1, 'str'),\n",
              " (53, '('),\n",
              " (1, 'test_list'),\n",
              " (53, ')'),\n",
              " (53, ')'),\n",
              " (4, '\\n'),\n",
              " (54, '\\xa0'),\n",
              " (54, '\\xa0'),\n",
              " (4, '\\n'),\n",
              " (55, '# using list comprehension '),\n",
              " (56, '\\n'),\n",
              " (55, '# to perform pair iteration in list\\xa0 '),\n",
              " (56, '\\n'),\n",
              " (1, 'var_2'),\n",
              " (53, '='),\n",
              " (53, '['),\n",
              " (53, '('),\n",
              " (53, '('),\n",
              " (1, 'var_3'),\n",
              " (53, ')'),\n",
              " (53, ','),\n",
              " (53, '('),\n",
              " (1, 'var_3'),\n",
              " (53, '+'),\n",
              " (2, '1'),\n",
              " (53, ')'),\n",
              " (53, '%'),\n",
              " (1, 'len'),\n",
              " (53, '('),\n",
              " (1, 'test_list'),\n",
              " (53, ')'),\n",
              " (53, ')'),\n",
              " (54, '\\xa0'),\n",
              " (56, '\\n'),\n",
              " (54, '\\xa0'),\n",
              " (54, '\\xa0'),\n",
              " (54, '\\xa0'),\n",
              " (54, '\\xa0'),\n",
              " (54, '\\xa0'),\n",
              " (54, '\\xa0'),\n",
              " (54, '\\xa0'),\n",
              " (54, '\\xa0'),\n",
              " (54, '\\xa0'),\n",
              " (1, 'for'),\n",
              " (1, 'var_3'),\n",
              " (1, 'in'),\n",
              " (1, 'range'),\n",
              " (53, '('),\n",
              " (1, 'len'),\n",
              " (53, '('),\n",
              " (1, 'test_list'),\n",
              " (53, ')'),\n",
              " (53, ')'),\n",
              " (53, ']'),\n",
              " (4, '\\n'),\n",
              " (54, '\\xa0'),\n",
              " (54, '\\xa0'),\n",
              " (4, '\\n'),\n",
              " (55, '# printing result '),\n",
              " (56, '\\n'),\n",
              " (1, 'print'),\n",
              " (53, '('),\n",
              " (3, '\"The pair list is : \"'),\n",
              " (53, '+'),\n",
              " (1, 'str'),\n",
              " (53, '('),\n",
              " (1, 'var_2'),\n",
              " (53, ')'),\n",
              " (53, ')'),\n",
              " (4, '\\n'),\n",
              " (0, '')]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data[0].Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sZedgyzu7cD",
        "outputId": "849b35fc-5253-4db0-9e81-0063fcb43145"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Input': ['given', 'an', 'array', 'of', 'integers', ',', 'return', 'indices', 'of', 'the', 'two', 'numbers', 'such', 'that', 'they', 'add', 'up', 'to', 'a', 'specific', 'target', '.'], 'Output': [(57, 'utf-8'), (56, '\\n'), (56, '\\n'), (56, '\\n'), (56, '\\n'), (56, '\\n'), (1, 'def'), (1, 'two_sum'), (53, '('), (1, 'nums'), (53, ','), (1, 'target'), (53, ')'), (53, ':'), (4, '\\n'), (5, '  '), (1, 'd'), (53, '='), (53, '{'), (53, '}'), (4, '\\n'), (1, 'for'), (1, 'var_1'), (53, ','), (1, 'num'), (1, 'in'), (1, 'enumerate'), (53, '('), (1, 'nums'), (53, ')'), (53, ':'), (4, '\\n'), (5, '    '), (1, 'if'), (1, 'target'), (53, '-'), (1, 'num'), (1, 'in'), (1, 'd'), (53, ':'), (4, '\\n'), (5, '        '), (1, 'return'), (53, '['), (1, 'd'), (53, '['), (1, 'target'), (53, '-'), (1, 'num'), (53, ']'), (53, ','), (1, 'var_1'), (53, ']'), (4, '\\n'), (6, ''), (1, 'else'), (53, ':'), (4, '\\n'), (5, '        '), (1, 'd'), (53, '['), (1, 'num'), (53, ']'), (53, '='), (1, 'var_1'), (4, '\\n'), (6, ''), (6, ''), (6, ''), (0, '')]}\n"
          ]
        }
      ],
      "source": [
        "print(vars(train_data.examples[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njS_msug6eQ3"
      },
      "source": [
        "# Transformer Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd-C0ntd8x1e"
      },
      "source": [
        "Our transformer can be understood in terms of its three components:\n",
        "1. An Encoder that encodes an input sequence into state representation vectors.\n",
        "2. An Attention mechanism that enables our Transformer model to focus on the right aspects of the sequential input stream. This is used repeatedly within both the encoder and the decoder to help them contextualize the input data.\n",
        "3. A Decoder that decodes the state representation vector to generate the target output sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSjhBKP59ntL"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgvD_MpkC2OS"
      },
      "source": [
        "![](https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/transformer-encoder.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQYN9WDA9Hh_"
      },
      "source": [
        "Our Encoder accepts a batch of source sequences and sequence masks as input. The source mask contains 1 in locations where the input sequence has valid values and 0 where the input sequence has <pad> values. This ensures that the attention mechanism within the encoder does not pay attention to <pad> values.\n",
        "\n",
        "We convert our source sequence tokens into embeddings(‘tok_embedding’) of ‘hid_dim’ length. Since were are not using any recurrent networks we need to tag each token with its positional indices in order to preserve sequential information. We create an indices tensor(i.e. ‘pos’) and convert this into an embedding(‘pos_embedding’) of length ‘hid_dim’. This is combined with the source sequence embeddings to create our initial Encoder Layer input tensor src. This src tensor is passed through a series of Encoder Layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NE6JimgOCz-w"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim,\n",
        "                 dropout, \n",
        "                 device,\n",
        "                 max_length = 1000):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim,\n",
        "                                                  dropout, \n",
        "                                                  device) \n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        \n",
        "        batch_size = src.shape[0]\n",
        "        src_len = src.shape[1]\n",
        "\n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        \n",
        "        #pos = [batch size, src len]\n",
        "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "            \n",
        "        #src = [batch size, src len, hid dim]\n",
        "            \n",
        "        return src"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1E12pQr9REU"
      },
      "source": [
        "An EncoderLayer is the basic building block of our Transformer’s Encoder component. Our src tensor along with its ‘src_mask’ are sent into a multi-head self-attention operation to help our model focus on the necessary aspects of the src tensor. The output from the attention operation is combined with the src tensor(via skip connection) and normalized to avoid vanishing/exploding gradients(during training). This combined output is sent into a PositionwiseFeedForwardLayer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LheiXWVFDEg"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim,  \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
        "                                                                     pf_dim, \n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        #src_mask = [batch size, 1, 1, src len] \n",
        "                \n",
        "        #self attention\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        #positionwise feedforward\n",
        "        _src = self.positionwise_feedforward(src)\n",
        "        \n",
        "        #dropout, residual and layer norm\n",
        "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        return src"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6h_Iqnk4Jg5k"
      },
      "source": [
        "![](https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/transformer-attention.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjEVmTzG9uZ4"
      },
      "source": [
        "A PositionwiseFeedForwardLayer takes the combined input and processes it further using two fully connected layers and a Relu activation function between them. This in combination with the src embedding is the final output of an EncoderLayer. This process repeats for each EncoderLayer block.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9w9xDUKL7LU"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
        "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "        \n",
        "        #x = [batch size, seq len, pf dim]\n",
        "        \n",
        "        x = self.fc_2(x)\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VroGxzNo-GGI"
      },
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3MtR2yd-Iaz"
      },
      "source": [
        "Attention is a mechanism that allows a model to focus on the necessary parts of the input sequence as per the demands of the task at hand.\n",
        "\n",
        "Researchers at google like to look at everything as an information retrieval problem. Therefore the [“Attention is all you need”](https://arxiv.org/abs/1706.03762) paper tries to look at attention in terms of “Query”, “Keys” and “Values”. A search engine accepts a “Query” and tries to match it up with Indices(i.e. Keys) in order to get appropriate values as results for the query. Similarly one can think of attention as a mechanism in which the query vector and key vector work towards getting the right attention weights(i.e. values).\n",
        "\n",
        "When multiple channels(or heads) of attention are applied in parallel to a single source, it is known as multi-head attention. This increases the learning capacity of the model and therefore leads to better results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZmeHfGhGzkN"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        assert hid_dim % n_heads == 0\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = hid_dim // n_heads\n",
        "        \n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "        \n",
        "    def forward(self, query, key, value, mask = None):\n",
        "        \n",
        "        batch_size = query.shape[0]\n",
        "        \n",
        "        #query = [batch size, query len, hid dim]\n",
        "        #key = [batch size, key len, hid dim]\n",
        "        #value = [batch size, value len, hid dim]\n",
        "                \n",
        "        Q = self.fc_q(query)\n",
        "        K = self.fc_k(key)\n",
        "        V = self.fc_v(value)\n",
        "        \n",
        "        #Q = [batch size, query len, hid dim]\n",
        "        #K = [batch size, key len, hid dim]\n",
        "        #V = [batch size, value len, hid dim]\n",
        "                \n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        \n",
        "        #Q = [batch size, n heads, query len, head dim]\n",
        "        #K = [batch size, n heads, key len, head dim]\n",
        "        #V = [batch size, n heads, value len, head dim]\n",
        "                \n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "        \n",
        "        #energy = [batch size, n heads, query len, key len]\n",
        "        \n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\n",
        "        \n",
        "        attention = torch.softmax(energy, dim = -1)\n",
        "                \n",
        "        #attention = [batch size, n heads, query len, key len]\n",
        "                \n",
        "        x = torch.matmul(self.dropout(attention), V)\n",
        "        \n",
        "        #x = [batch size, n heads, query len, head dim]\n",
        "        \n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        \n",
        "        #x = [batch size, query len, n heads, head dim]\n",
        "        \n",
        "        x = x.view(batch_size, -1, self.hid_dim)\n",
        "        \n",
        "        #x = [batch size, query len, hid dim]\n",
        "        \n",
        "        x = self.fc_o(x)\n",
        "        \n",
        "        #x = [batch size, query len, hid dim]\n",
        "        \n",
        "        return x, attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBFqGg5z-o_r"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbTr7YPSMRpC"
      },
      "source": [
        "![](https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/transformer-decoder.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaHaOGNm_Isc"
      },
      "source": [
        "The architecture of a Decoder is very similar to that of the encoder with the significant differences resulting from the presence of input from two sources, the target sequence and the state representation vector from the encoder. Much like how we had an EncoderLayer block for Encoder, we will be having a DecoderLayer that accepts as input the combination of the embedding from the target token sequence(tok_embedding) and embedding of positional indices for these tokens. And as mentioned earlier, the encoder’s output also acts as one of the inputs to the DecoderLayer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWBMMF45MMNS"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 output_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 dropout, \n",
        "                 device,\n",
        "                 max_length = 10000):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim, \n",
        "                                                  dropout, \n",
        "                                                  device)\n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch size, trg len]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "                \n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "                            \n",
        "        #pos = [batch size, trg len]\n",
        "\n",
        "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
        "                \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        output = self.fc_out(trg)\n",
        "        \n",
        "        #output = [batch size, trg len, output dim]\n",
        "            \n",
        "        return output, attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcK1-4qZ_WBM"
      },
      "source": [
        "The DecoderLayer forms the building block of our Transformer’s decoder. Each DecoderLayer involves two attention operations:\n",
        "1. Self-attention on trg embedding.\n",
        "2. Multi-head attention operation that uses the trg as query vector and the encoder outputs act as the key and value vectors.\n",
        "\n",
        "The presence of an extra Multi-head attention operation differentiates the DecoderLayer from an EncoderLayer.\n",
        "\n",
        "The attention outputs from self-attention are normalized and combined with the trg embedding using a residual connection. This is then sent into the multi-head attention operation along with the encoder outputs. The attention layer outputs are then combined with the trg input again and normalized before sending it into the position-wise feedforward layer to generate the final outputs of the DecoderLayer.\n",
        "\n",
        "The purpose of all normalization operations is to prevent vanishing/exploding gradients during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMEr1IFUMxco"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
        "                                                                     pf_dim, \n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        \n",
        "        #self attention\n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
        "            \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "            \n",
        "        #encoder attention\n",
        "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
        "        # query, key, value\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
        "                    \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        \n",
        "        #positionwise feedforward\n",
        "        _trg = self.positionwise_feedforward(trg)\n",
        "        \n",
        "        #dropout, residual and layer norm\n",
        "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return trg, attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udpPhQ2UN8oQ"
      },
      "source": [
        "The main class that implements a transformer for seq2seq problems is given below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dr3Mg8OGN6ul"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, \n",
        "                 encoder, \n",
        "                 decoder, \n",
        "                 src_pad_idx, \n",
        "                 trg_pad_idx, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "        \n",
        "    def make_src_mask(self, src):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        \n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "\n",
        "        return src_mask\n",
        "    \n",
        "    def make_trg_mask(self, trg):\n",
        "        \n",
        "        #trg = [batch size, trg len]\n",
        "        \n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        \n",
        "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
        "        \n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
        "        \n",
        "        #trg_sub_mask = [trg len, trg len]\n",
        "            \n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "        \n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        \n",
        "        return trg_mask\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #trg = [batch size, trg len]\n",
        "                \n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        \n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        \n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        \n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "                \n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        #output = [batch size, trg len, output dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return output, attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvrK6zbF_2Fs"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zsZjSSWOSHc"
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = len(Input.vocab)\n",
        "OUTPUT_DIM = len(Output.vocab)\n",
        "HID_DIM = 16\n",
        "ENC_LAYERS = 3\n",
        "DEC_LAYERS = 3\n",
        "ENC_HEADS = 16\n",
        "DEC_HEADS = 16\n",
        "ENC_PF_DIM = 512\n",
        "DEC_PF_DIM = 512\n",
        "ENC_DROPOUT = 0.1\n",
        "DEC_DROPOUT = 0.1\n",
        "\n",
        "enc = Encoder(INPUT_DIM, \n",
        "              HID_DIM, \n",
        "              ENC_LAYERS, \n",
        "              ENC_HEADS, \n",
        "              ENC_PF_DIM, \n",
        "              ENC_DROPOUT, \n",
        "              device)\n",
        "\n",
        "dec = Decoder(OUTPUT_DIM, \n",
        "              HID_DIM, \n",
        "              DEC_LAYERS, \n",
        "              DEC_HEADS, \n",
        "              DEC_PF_DIM, \n",
        "              DEC_DROPOUT, \n",
        "              device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3vG-tI737e6",
        "outputId": "d82e46c4-f4e1-4d7f-b1fe-e9e3695de674"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "23920"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(Output.vocab.__dict__['freqs'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYVZYDVcOUGK"
      },
      "outputs": [],
      "source": [
        "SRC_PAD_IDX = Input.vocab.stoi[Input.pad_token]\n",
        "TRG_PAD_IDX = Output.vocab.stoi[Output.pad_token]\n",
        "\n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qd0ePzj0OzLa",
        "outputId": "18c4f2e7-6525-406c-c880-5bfbff18eba3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 1,210,132 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmZ0hyo8O0vE"
      },
      "outputs": [],
      "source": [
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRtAM9Y4O2N2"
      },
      "outputs": [],
      "source": [
        "model.apply(initialize_weights);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEpApG3YO3ZE"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 0.0005\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95cvaGRg_-Ml"
      },
      "source": [
        "## Loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bs2XdrveCtDD"
      },
      "source": [
        "We have used augmentations in our dataset to mask variable literals. This means that our model can predict a variety of values for a particular variable and all of them are correct as long as the predictions are consistent through the code. This would mean that our training labels are not very certain and hence it would make more sense to treat them to be correct with probability ```1- smooth_eps``` and incorrect otherwise. This is what label smoothening enables us to do. The following is the implementation of CrossEntropyLoss with label smoothening."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNfxmsLxD7yb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CrossEntropyLoss(nn.CrossEntropyLoss):\n",
        "    \"\"\"CrossEntropyLoss - with ability to recieve distrbution as targets, and optional label smoothing\"\"\"\n",
        "\n",
        "    def __init__(self, weight=None, ignore_index=-100, reduction='mean', smooth_eps=None, smooth_dist=None, from_logits=True):\n",
        "        super(CrossEntropyLoss, self).__init__(weight=weight,\n",
        "                                               ignore_index=ignore_index, reduction=reduction)\n",
        "        self.smooth_eps = smooth_eps\n",
        "        self.smooth_dist = smooth_dist\n",
        "        self.from_logits = from_logits\n",
        "\n",
        "    def forward(self, input, target, smooth_dist=None):\n",
        "        if smooth_dist is None:\n",
        "            smooth_dist = self.smooth_dist\n",
        "        return cross_entropy(input, target, weight=self.weight, ignore_index=self.ignore_index,\n",
        "                             reduction=self.reduction, smooth_eps=self.smooth_eps,\n",
        "                             smooth_dist=smooth_dist, from_logits=self.from_logits)\n",
        "\n",
        "\n",
        "def cross_entropy(inputs, target, weight=None, ignore_index=-100, reduction='mean',\n",
        "                  smooth_eps=None, smooth_dist=None, from_logits=True):\n",
        "    \"\"\"cross entropy loss, with support for target distributions and label smoothing https://arxiv.org/abs/1512.00567\"\"\"\n",
        "    smooth_eps = smooth_eps or 0\n",
        "\n",
        "    # ordinary log-liklihood - use cross_entropy from nn\n",
        "    if _is_long(target) and smooth_eps == 0:\n",
        "        if from_logits:\n",
        "            return F.cross_entropy(inputs, target, weight, ignore_index=ignore_index, reduction=reduction)\n",
        "        else:\n",
        "            return F.nll_loss(inputs, target, weight, ignore_index=ignore_index, reduction=reduction)\n",
        "\n",
        "    if from_logits:\n",
        "        # log-softmax of inputs\n",
        "        lsm = F.log_softmax(inputs, dim=-1)\n",
        "    else:\n",
        "        lsm = inputs\n",
        "\n",
        "    masked_indices = None\n",
        "    num_classes = inputs.size(-1)\n",
        "\n",
        "    if _is_long(target) and ignore_index >= 0:\n",
        "        masked_indices = target.eq(ignore_index)\n",
        "\n",
        "    if smooth_eps > 0 and smooth_dist is not None:\n",
        "        if _is_long(target):\n",
        "            target = onehot(target, num_classes).type_as(inputs)\n",
        "        if smooth_dist.dim() < target.dim():\n",
        "            smooth_dist = smooth_dist.unsqueeze(0)\n",
        "        target.lerp_(smooth_dist, smooth_eps)\n",
        "\n",
        "    if weight is not None:\n",
        "        lsm = lsm * weight.unsqueeze(0)\n",
        "\n",
        "    if _is_long(target):\n",
        "        eps_sum = smooth_eps / num_classes\n",
        "        eps_nll = 1. - eps_sum - smooth_eps\n",
        "        likelihood = lsm.gather(dim=-1, index=target.unsqueeze(-1)).squeeze(-1)\n",
        "        loss = -(eps_nll * likelihood + eps_sum * lsm.sum(-1))\n",
        "    else:\n",
        "        loss = -(target * lsm).sum(-1)\n",
        "\n",
        "    if masked_indices is not None:\n",
        "        loss.masked_fill_(masked_indices, 0)\n",
        "\n",
        "    if reduction == 'sum':\n",
        "        loss = loss.sum()\n",
        "    elif reduction == 'mean':\n",
        "        if masked_indices is None:\n",
        "            loss = loss.mean()\n",
        "        else:\n",
        "            loss = loss.sum() / float(loss.size(0) - masked_indices.sum())\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def onehot(indexes, N=None, ignore_index=None):\n",
        "    \"\"\"\n",
        "    Creates a one-representation of indexes with N possible entries\n",
        "    if N is not specified, it will suit the maximum index appearing.\n",
        "    indexes is a long-tensor of indexes\n",
        "    ignore_index will be zero in onehot representation\n",
        "    \"\"\"\n",
        "    if N is None:\n",
        "        N = indexes.max() + 1\n",
        "    sz = list(indexes.size())\n",
        "    output = indexes.new().byte().resize_(*sz, N).zero_()\n",
        "    output.scatter_(-1, indexes.unsqueeze(-1), 1)\n",
        "    if ignore_index is not None and ignore_index >= 0:\n",
        "        output.masked_fill_(indexes.eq(ignore_index).unsqueeze(-1), 0)\n",
        "    return output\n",
        "\n",
        "def _is_long(x):\n",
        "    if hasattr(x, 'data'):\n",
        "        x = x.data\n",
        "    return isinstance(x, torch.LongTensor) or isinstance(x, torch.cuda.LongTensor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnWtafKlAhb_"
      },
      "outputs": [],
      "source": [
        "def maskNLLLoss(inp, target, mask):\n",
        "    # print(inp.shape, target.shape, mask.sum())\n",
        "    nTotal = mask.sum()\n",
        "    crossEntropy = CrossEntropyLoss(ignore_index = TRG_PAD_IDX, smooth_eps=0.20)\n",
        "    loss = crossEntropy(inp, target)\n",
        "    loss = loss.to(device)\n",
        "    return loss, nTotal.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9Dy_wWrO46l"
      },
      "outputs": [],
      "source": [
        "criterion = maskNLLLoss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQGdcEFmAxlO"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itmoGDi_tN74"
      },
      "source": [
        "In order to re-apply our augmentations differently in every epoch we re-create our dataset and dataloaders at the start of each epoch. This regularizes our training process and helps us come up with better models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycBBiEpuO6cG"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def make_trg_mask(trg):\n",
        "        \n",
        "        #trg = [batch size, trg len]\n",
        "        \n",
        "        trg_pad_mask = (trg != TRG_PAD_IDX).unsqueeze(1).unsqueeze(2)\n",
        "        \n",
        "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
        "        \n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = device)).bool()\n",
        "        \n",
        "        #trg_sub_mask = [trg len, trg len]\n",
        "            \n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "        \n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        \n",
        "        return trg_mask\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    n_totals = 0\n",
        "    print_losses = []\n",
        "    for i, batch in tqdm(enumerate(iterator), total=len(iterator)):\n",
        "        # print(batch)\n",
        "        loss = 0\n",
        "        src = batch.Input.permute(1, 0)\n",
        "        trg = batch.Output.permute(1, 0)\n",
        "        trg_mask = make_trg_mask(trg)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output, _ = model(src, trg[:,:-1])\n",
        "                \n",
        "        #output = [batch size, trg len - 1, output dim]\n",
        "        #trg = [batch size, trg len]\n",
        "            \n",
        "        output_dim = output.shape[-1]\n",
        "            \n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "                \n",
        "        #output = [batch size * trg len - 1, output dim]\n",
        "        #trg = [batch size * trg len - 1]\n",
        "            \n",
        "        mask_loss, nTotal = criterion(output, trg, trg_mask)\n",
        "        \n",
        "        mask_loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        print_losses.append(mask_loss.item() * nTotal)\n",
        "        n_totals += nTotal\n",
        "\n",
        "\n",
        "        \n",
        "    return sum(print_losses) / n_totals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zi3Ev8gaO79_"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    n_totals = 0\n",
        "    print_losses = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in tqdm(enumerate(iterator), total=len(iterator)):\n",
        "\n",
        "            src = batch.Input.permute(1, 0)\n",
        "            trg = batch.Output.permute(1, 0)\n",
        "            trg_mask = make_trg_mask(trg)\n",
        "\n",
        "            output, _ = model(src, trg[:,:-1])\n",
        "            \n",
        "            #output = [batch size, trg len - 1, output dim]\n",
        "            #trg = [batch size, trg len]\n",
        "            \n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "            \n",
        "            #output = [batch size * trg len - 1, output dim]\n",
        "            #trg = [batch size * trg len - 1]\n",
        "            \n",
        "            mask_loss, nTotal = criterion(output, trg, trg_mask)\n",
        "\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "\n",
        "        \n",
        "    return sum(print_losses) / n_totals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuB4JqQRO9Wg"
      },
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9hnkbpugxSn"
      },
      "outputs": [],
      "source": [
        "model_saved_path = model_saved_fol_path+'model.pt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "aax76Ie4O_Cr",
        "outputId": "803000ba-38b9-4a2a-870d-5bc1a937d502"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [02:01<00:00, 29.71it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 152.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 2m 5s\n",
            "\tTrain Loss: 6.170 | Train PPL: 478.031\n",
            "\t Val. Loss: 5.283 |  Val. PPL: 196.918\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:57<00:00, 30.53it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 152.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 02 | Time: 2m 2s\n",
            "\tTrain Loss: 5.423 | Train PPL: 226.545\n",
            "\t Val. Loss: 5.119 |  Val. PPL: 167.219\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:56<00:00, 30.89it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 156.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 03 | Time: 2m 3s\n",
            "\tTrain Loss: 5.257 | Train PPL: 191.883\n",
            "\t Val. Loss: 5.003 |  Val. PPL: 148.900\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:55<00:00, 31.02it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 155.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 04 | Time: 2m 0s\n",
            "\tTrain Loss: 5.181 | Train PPL: 177.919\n",
            "\t Val. Loss: 4.950 |  Val. PPL: 141.244\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:55<00:00, 31.10it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 150.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 05 | Time: 1m 59s\n",
            "\tTrain Loss: 5.141 | Train PPL: 170.870\n",
            "\t Val. Loss: 4.934 |  Val. PPL: 138.872\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:57<00:00, 30.64it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 149.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 06 | Time: 2m 1s\n",
            "\tTrain Loss: 5.101 | Train PPL: 164.211\n",
            "\t Val. Loss: 4.899 |  Val. PPL: 134.174\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [02:00<00:00, 29.97it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 150.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 07 | Time: 2m 4s\n",
            "\tTrain Loss: 5.080 | Train PPL: 160.788\n",
            "\t Val. Loss: 4.917 |  Val. PPL: 136.582\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:57<00:00, 30.65it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 153.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 08 | Time: 2m 1s\n",
            "\tTrain Loss: 5.051 | Train PPL: 156.181\n",
            "\t Val. Loss: 4.880 |  Val. PPL: 131.690\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:57<00:00, 30.65it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 153.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 09 | Time: 2m 1s\n",
            "\tTrain Loss: 5.021 | Train PPL: 151.544\n",
            "\t Val. Loss: 4.845 |  Val. PPL: 127.133\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:56<00:00, 30.88it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 152.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 10 | Time: 2m 0s\n",
            "\tTrain Loss: 5.011 | Train PPL: 150.114\n",
            "\t Val. Loss: 4.860 |  Val. PPL: 128.970\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:56<00:00, 30.78it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 150.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 11 | Time: 2m 3s\n",
            "\tTrain Loss: 5.002 | Train PPL: 148.649\n",
            "\t Val. Loss: 4.844 |  Val. PPL: 126.970\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:57<00:00, 30.70it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 150.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 12 | Time: 2m 1s\n",
            "\tTrain Loss: 4.981 | Train PPL: 145.576\n",
            "\t Val. Loss: 4.870 |  Val. PPL: 130.297\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:57<00:00, 30.55it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 151.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 13 | Time: 2m 2s\n",
            "\tTrain Loss: 4.987 | Train PPL: 146.564\n",
            "\t Val. Loss: 4.874 |  Val. PPL: 130.818\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:59<00:00, 30.13it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 149.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 14 | Time: 2m 3s\n",
            "\tTrain Loss: 4.968 | Train PPL: 143.802\n",
            "\t Val. Loss: 4.854 |  Val. PPL: 128.226\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:59<00:00, 30.23it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 148.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 15 | Time: 2m 3s\n",
            "\tTrain Loss: 4.968 | Train PPL: 143.797\n",
            "\t Val. Loss: 4.787 |  Val. PPL: 119.917\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:58<00:00, 30.36it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 141.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 16 | Time: 2m 2s\n",
            "\tTrain Loss: 4.943 | Train PPL: 140.173\n",
            "\t Val. Loss: 4.858 |  Val. PPL: 128.754\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:57<00:00, 30.58it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 151.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 17 | Time: 2m 1s\n",
            "\tTrain Loss: 4.944 | Train PPL: 140.344\n",
            "\t Val. Loss: 4.827 |  Val. PPL: 124.787\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:56<00:00, 30.79it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 153.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 18 | Time: 2m 3s\n",
            "\tTrain Loss: 4.936 | Train PPL: 139.246\n",
            "\t Val. Loss: 4.817 |  Val. PPL: 123.648\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:56<00:00, 30.88it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 144.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 19 | Time: 2m 0s\n",
            "\tTrain Loss: 4.930 | Train PPL: 138.351\n",
            "\t Val. Loss: 4.801 |  Val. PPL: 121.689\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:56<00:00, 30.75it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 151.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 20 | Time: 2m 1s\n",
            "\tTrain Loss: 4.915 | Train PPL: 136.299\n",
            "\t Val. Loss: 4.828 |  Val. PPL: 124.961\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:56<00:00, 30.78it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 152.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 21 | Time: 2m 1s\n",
            "\tTrain Loss: 4.911 | Train PPL: 135.744\n",
            "\t Val. Loss: 4.804 |  Val. PPL: 121.997\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:57<00:00, 30.55it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 149.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 22 | Time: 2m 1s\n",
            "\tTrain Loss: 4.903 | Train PPL: 134.675\n",
            "\t Val. Loss: 4.780 |  Val. PPL: 119.132\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:57<00:00, 30.59it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 148.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 23 | Time: 2m 1s\n",
            "\tTrain Loss: 4.894 | Train PPL: 133.539\n",
            "\t Val. Loss: 4.816 |  Val. PPL: 123.434\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:58<00:00, 30.40it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 152.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 24 | Time: 2m 2s\n",
            "\tTrain Loss: 4.893 | Train PPL: 133.414\n",
            "\t Val. Loss: 4.813 |  Val. PPL: 123.136\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:57<00:00, 30.68it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 151.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 25 | Time: 2m 3s\n",
            "\tTrain Loss: 4.897 | Train PPL: 133.830\n",
            "\t Val. Loss: 4.761 |  Val. PPL: 116.908\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:57<00:00, 30.55it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 147.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 26 | Time: 2m 2s\n",
            "\tTrain Loss: 4.891 | Train PPL: 133.142\n",
            "\t Val. Loss: 4.794 |  Val. PPL: 120.756\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:58<00:00, 30.38it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 150.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 27 | Time: 2m 2s\n",
            "\tTrain Loss: 4.887 | Train PPL: 132.600\n",
            "\t Val. Loss: 4.793 |  Val. PPL: 120.665\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:57<00:00, 30.55it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 151.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 28 | Time: 2m 2s\n",
            "\tTrain Loss: 4.880 | Train PPL: 131.695\n",
            "\t Val. Loss: 4.802 |  Val. PPL: 121.809\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:58<00:00, 30.43it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 150.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 29 | Time: 2m 2s\n",
            "\tTrain Loss: 4.888 | Train PPL: 132.679\n",
            "\t Val. Loss: 4.825 |  Val. PPL: 124.546\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:57<00:00, 30.53it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 150.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 30 | Time: 2m 2s\n",
            "\tTrain Loss: 4.883 | Train PPL: 132.075\n",
            "\t Val. Loss: 4.815 |  Val. PPL: 123.299\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:59<00:00, 30.04it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 152.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 31 | Time: 2m 3s\n",
            "\tTrain Loss: 4.876 | Train PPL: 131.123\n",
            "\t Val. Loss: 4.830 |  Val. PPL: 125.158\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:57<00:00, 30.54it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 151.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 32 | Time: 2m 1s\n",
            "\tTrain Loss: 4.870 | Train PPL: 130.332\n",
            "\t Val. Loss: 4.800 |  Val. PPL: 121.557\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:56<00:00, 30.84it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 152.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 33 | Time: 2m 3s\n",
            "\tTrain Loss: 4.861 | Train PPL: 129.161\n",
            "\t Val. Loss: 4.771 |  Val. PPL: 118.068\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:55<00:00, 31.07it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 155.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 34 | Time: 1m 59s\n",
            "\tTrain Loss: 4.851 | Train PPL: 127.843\n",
            "\t Val. Loss: 4.788 |  Val. PPL: 120.040\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:54<00:00, 31.38it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 150.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 35 | Time: 1m 58s\n",
            "\tTrain Loss: 4.854 | Train PPL: 128.293\n",
            "\t Val. Loss: 4.783 |  Val. PPL: 119.460\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:54<00:00, 31.32it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 153.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 36 | Time: 1m 58s\n",
            "\tTrain Loss: 4.856 | Train PPL: 128.571\n",
            "\t Val. Loss: 4.759 |  Val. PPL: 116.623\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:54<00:00, 31.53it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 154.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 37 | Time: 1m 58s\n",
            "\tTrain Loss: 4.849 | Train PPL: 127.602\n",
            "\t Val. Loss: 4.771 |  Val. PPL: 118.058\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:54<00:00, 31.45it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 152.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 38 | Time: 1m 58s\n",
            "\tTrain Loss: 4.846 | Train PPL: 127.172\n",
            "\t Val. Loss: 4.771 |  Val. PPL: 118.048\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:54<00:00, 31.38it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 152.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 39 | Time: 1m 58s\n",
            "\tTrain Loss: 4.853 | Train PPL: 128.089\n",
            "\t Val. Loss: 4.797 |  Val. PPL: 121.092\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:54<00:00, 31.50it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 155.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 40 | Time: 2m 0s\n",
            "\tTrain Loss: 4.840 | Train PPL: 126.531\n",
            "\t Val. Loss: 4.776 |  Val. PPL: 118.635\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:54<00:00, 31.38it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 152.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 41 | Time: 1m 58s\n",
            "\tTrain Loss: 4.840 | Train PPL: 126.475\n",
            "\t Val. Loss: 4.757 |  Val. PPL: 116.438\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:54<00:00, 31.35it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 154.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 42 | Time: 1m 58s\n",
            "\tTrain Loss: 4.836 | Train PPL: 126.011\n",
            "\t Val. Loss: 4.798 |  Val. PPL: 121.288\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:54<00:00, 31.49it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 154.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 43 | Time: 1m 58s\n",
            "\tTrain Loss: 4.834 | Train PPL: 125.683\n",
            "\t Val. Loss: 4.762 |  Val. PPL: 116.978\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:54<00:00, 31.52it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 156.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 44 | Time: 1m 58s\n",
            "\tTrain Loss: 4.828 | Train PPL: 124.973\n",
            "\t Val. Loss: 4.772 |  Val. PPL: 118.183\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:54<00:00, 31.47it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 153.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 45 | Time: 1m 58s\n",
            "\tTrain Loss: 4.831 | Train PPL: 125.372\n",
            "\t Val. Loss: 4.761 |  Val. PPL: 116.914\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:54<00:00, 31.43it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 152.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 46 | Time: 1m 58s\n",
            "\tTrain Loss: 4.826 | Train PPL: 124.656\n",
            "\t Val. Loss: 4.752 |  Val. PPL: 115.827\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:56<00:00, 31.02it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 154.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 47 | Time: 2m 2s\n",
            "\tTrain Loss: 4.828 | Train PPL: 124.932\n",
            "\t Val. Loss: 4.777 |  Val. PPL: 118.742\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:56<00:00, 30.81it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 151.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 48 | Time: 2m 0s\n",
            "\tTrain Loss: 4.821 | Train PPL: 124.032\n",
            "\t Val. Loss: 4.798 |  Val. PPL: 121.281\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:55<00:00, 31.07it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 152.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 49 | Time: 1m 59s\n",
            "\tTrain Loss: 4.820 | Train PPL: 124.017\n",
            "\t Val. Loss: 4.764 |  Val. PPL: 117.230\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:55<00:00, 31.15it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 153.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 50 | Time: 1m 59s\n",
            "\tTrain Loss: 4.820 | Train PPL: 123.959\n",
            "\t Val. Loss: 4.784 |  Val. PPL: 119.626\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:55<00:00, 31.04it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 152.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 51 | Time: 2m 0s\n",
            "\tTrain Loss: 4.808 | Train PPL: 122.476\n",
            "\t Val. Loss: 4.771 |  Val. PPL: 118.087\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:54<00:00, 31.44it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 155.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 52 | Time: 1m 58s\n",
            "\tTrain Loss: 4.822 | Train PPL: 124.155\n",
            "\t Val. Loss: 4.734 |  Val. PPL: 113.722\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:54<00:00, 31.42it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 149.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 53 | Time: 1m 58s\n",
            "\tTrain Loss: 4.821 | Train PPL: 124.030\n",
            "\t Val. Loss: 4.730 |  Val. PPL: 113.307\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:55<00:00, 31.23it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 150.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 54 | Time: 1m 59s\n",
            "\tTrain Loss: 4.829 | Train PPL: 125.098\n",
            "\t Val. Loss: 4.723 |  Val. PPL: 112.509\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:56<00:00, 30.96it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 152.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 55 | Time: 2m 2s\n",
            "\tTrain Loss: 4.812 | Train PPL: 122.973\n",
            "\t Val. Loss: 4.766 |  Val. PPL: 117.444\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:54<00:00, 31.37it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 151.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 56 | Time: 1m 58s\n",
            "\tTrain Loss: 4.811 | Train PPL: 122.824\n",
            "\t Val. Loss: 4.780 |  Val. PPL: 119.054\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:56<00:00, 31.00it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 152.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 57 | Time: 2m 0s\n",
            "\tTrain Loss: 4.812 | Train PPL: 123.012\n",
            "\t Val. Loss: 4.797 |  Val. PPL: 121.121\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:56<00:00, 30.93it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 151.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 58 | Time: 2m 0s\n",
            "\tTrain Loss: 4.817 | Train PPL: 123.624\n",
            "\t Val. Loss: 4.757 |  Val. PPL: 116.453\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:55<00:00, 31.18it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 150.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 59 | Time: 1m 59s\n",
            "\tTrain Loss: 4.808 | Train PPL: 122.499\n",
            "\t Val. Loss: 4.747 |  Val. PPL: 115.251\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:55<00:00, 31.28it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 155.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 60 | Time: 1m 59s\n",
            "\tTrain Loss: 4.808 | Train PPL: 122.428\n",
            "\t Val. Loss: 4.754 |  Val. PPL: 116.036\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:55<00:00, 31.06it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 151.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 61 | Time: 2m 0s\n",
            "\tTrain Loss: 4.814 | Train PPL: 123.167\n",
            "\t Val. Loss: 4.739 |  Val. PPL: 114.349\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:56<00:00, 30.94it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 155.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 62 | Time: 2m 2s\n",
            "\tTrain Loss: 4.797 | Train PPL: 121.153\n",
            "\t Val. Loss: 4.781 |  Val. PPL: 119.175\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:55<00:00, 31.19it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 151.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 63 | Time: 1m 59s\n",
            "\tTrain Loss: 4.809 | Train PPL: 122.565\n",
            "\t Val. Loss: 4.734 |  Val. PPL: 113.755\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:55<00:00, 31.21it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 152.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 64 | Time: 1m 59s\n",
            "\tTrain Loss: 4.804 | Train PPL: 121.978\n",
            "\t Val. Loss: 4.735 |  Val. PPL: 113.829\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:54<00:00, 31.42it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 154.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 65 | Time: 1m 58s\n",
            "\tTrain Loss: 4.796 | Train PPL: 120.995\n",
            "\t Val. Loss: 4.739 |  Val. PPL: 114.354\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:54<00:00, 31.44it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 158.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 66 | Time: 1m 58s\n",
            "\tTrain Loss: 4.798 | Train PPL: 121.263\n",
            "\t Val. Loss: 4.726 |  Val. PPL: 112.849\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:55<00:00, 31.27it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 154.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 67 | Time: 1m 59s\n",
            "\tTrain Loss: 4.787 | Train PPL: 119.982\n",
            "\t Val. Loss: 4.737 |  Val. PPL: 114.067\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:56<00:00, 31.00it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 152.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 68 | Time: 2m 0s\n",
            "\tTrain Loss: 4.795 | Train PPL: 120.918\n",
            "\t Val. Loss: 4.771 |  Val. PPL: 118.063\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:55<00:00, 31.24it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 154.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 69 | Time: 2m 1s\n",
            "\tTrain Loss: 4.796 | Train PPL: 121.071\n",
            "\t Val. Loss: 4.736 |  Val. PPL: 114.005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:54<00:00, 31.41it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 151.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 70 | Time: 1m 58s\n",
            "\tTrain Loss: 4.797 | Train PPL: 121.150\n",
            "\t Val. Loss: 4.744 |  Val. PPL: 114.868\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:54<00:00, 31.52it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 153.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 71 | Time: 1m 58s\n",
            "\tTrain Loss: 4.790 | Train PPL: 120.329\n",
            "\t Val. Loss: 4.762 |  Val. PPL: 117.011\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:54<00:00, 31.51it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 155.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 72 | Time: 1m 58s\n",
            "\tTrain Loss: 4.800 | Train PPL: 121.465\n",
            "\t Val. Loss: 4.750 |  Val. PPL: 115.603\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:54<00:00, 31.29it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 151.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 73 | Time: 1m 59s\n",
            "\tTrain Loss: 4.803 | Train PPL: 121.869\n",
            "\t Val. Loss: 4.731 |  Val. PPL: 113.448\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:55<00:00, 31.19it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 152.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 74 | Time: 1m 59s\n",
            "\tTrain Loss: 4.780 | Train PPL: 119.047\n",
            "\t Val. Loss: 4.748 |  Val. PPL: 115.385\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:55<00:00, 31.23it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 151.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 75 | Time: 1m 59s\n",
            "\tTrain Loss: 4.790 | Train PPL: 120.273\n",
            "\t Val. Loss: 4.760 |  Val. PPL: 116.760\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:56<00:00, 30.96it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 149.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 76 | Time: 2m 0s\n",
            "\tTrain Loss: 4.787 | Train PPL: 119.936\n",
            "\t Val. Loss: 4.726 |  Val. PPL: 112.853\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:55<00:00, 31.10it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 150.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 77 | Time: 2m 2s\n",
            "\tTrain Loss: 4.792 | Train PPL: 120.592\n",
            "\t Val. Loss: 4.742 |  Val. PPL: 114.710\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:55<00:00, 31.24it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 153.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 78 | Time: 1m 59s\n",
            "\tTrain Loss: 4.783 | Train PPL: 119.485\n",
            "\t Val. Loss: 4.765 |  Val. PPL: 117.354\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:55<00:00, 31.26it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 153.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 79 | Time: 1m 59s\n",
            "\tTrain Loss: 4.772 | Train PPL: 118.176\n",
            "\t Val. Loss: 4.770 |  Val. PPL: 117.889\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:55<00:00, 31.21it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 154.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 80 | Time: 1m 59s\n",
            "\tTrain Loss: 4.790 | Train PPL: 120.256\n",
            "\t Val. Loss: 4.767 |  Val. PPL: 117.570\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:55<00:00, 31.16it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 152.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 81 | Time: 1m 59s\n",
            "\tTrain Loss: 4.787 | Train PPL: 119.929\n",
            "\t Val. Loss: 4.747 |  Val. PPL: 115.274\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:56<00:00, 30.78it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 153.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 82 | Time: 2m 1s\n",
            "\tTrain Loss: 4.771 | Train PPL: 117.984\n",
            "\t Val. Loss: 4.748 |  Val. PPL: 115.370\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:55<00:00, 31.13it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 147.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 83 | Time: 1m 59s\n",
            "\tTrain Loss: 4.791 | Train PPL: 120.373\n",
            "\t Val. Loss: 4.754 |  Val. PPL: 116.062\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:55<00:00, 31.26it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 154.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 84 | Time: 2m 1s\n",
            "\tTrain Loss: 4.790 | Train PPL: 120.257\n",
            "\t Val. Loss: 4.723 |  Val. PPL: 112.547\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:54<00:00, 31.37it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 153.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 85 | Time: 1m 58s\n",
            "\tTrain Loss: 4.791 | Train PPL: 120.447\n",
            "\t Val. Loss: 4.735 |  Val. PPL: 113.837\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:54<00:00, 31.40it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 154.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 86 | Time: 1m 58s\n",
            "\tTrain Loss: 4.785 | Train PPL: 119.704\n",
            "\t Val. Loss: 4.749 |  Val. PPL: 115.448\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:55<00:00, 31.16it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 153.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 87 | Time: 1m 59s\n",
            "\tTrain Loss: 4.778 | Train PPL: 118.845\n",
            "\t Val. Loss: 4.696 |  Val. PPL: 109.491\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:54<00:00, 31.38it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 155.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 88 | Time: 1m 58s\n",
            "\tTrain Loss: 4.765 | Train PPL: 117.314\n",
            "\t Val. Loss: 4.728 |  Val. PPL: 113.015\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:55<00:00, 31.11it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 155.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 89 | Time: 1m 59s\n",
            "\tTrain Loss: 4.768 | Train PPL: 117.649\n",
            "\t Val. Loss: 4.741 |  Val. PPL: 114.599\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:54<00:00, 31.49it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 153.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 90 | Time: 1m 58s\n",
            "\tTrain Loss: 4.776 | Train PPL: 118.595\n",
            "\t Val. Loss: 4.726 |  Val. PPL: 112.847\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:54<00:00, 31.36it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 153.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 91 | Time: 1m 58s\n",
            "\tTrain Loss: 4.781 | Train PPL: 119.270\n",
            "\t Val. Loss: 4.744 |  Val. PPL: 114.945\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:54<00:00, 31.48it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 155.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 92 | Time: 2m 0s\n",
            "\tTrain Loss: 4.771 | Train PPL: 118.066\n",
            "\t Val. Loss: 4.767 |  Val. PPL: 117.599\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:54<00:00, 31.46it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 154.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 93 | Time: 1m 58s\n",
            "\tTrain Loss: 4.781 | Train PPL: 119.250\n",
            "\t Val. Loss: 4.712 |  Val. PPL: 111.283\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:55<00:00, 31.18it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 152.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 94 | Time: 1m 59s\n",
            "\tTrain Loss: 4.768 | Train PPL: 117.646\n",
            "\t Val. Loss: 4.742 |  Val. PPL: 114.635\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:54<00:00, 31.29it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 155.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 95 | Time: 1m 59s\n",
            "\tTrain Loss: 4.764 | Train PPL: 117.255\n",
            "\t Val. Loss: 4.712 |  Val. PPL: 111.312\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:55<00:00, 31.09it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 154.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 96 | Time: 1m 59s\n",
            "\tTrain Loss: 4.771 | Train PPL: 117.999\n",
            "\t Val. Loss: 4.729 |  Val. PPL: 113.187\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:55<00:00, 31.26it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 154.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 97 | Time: 1m 59s\n",
            "\tTrain Loss: 4.775 | Train PPL: 118.534\n",
            "\t Val. Loss: 4.750 |  Val. PPL: 115.559\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:54<00:00, 31.35it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 150.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 98 | Time: 1m 58s\n",
            "\tTrain Loss: 4.751 | Train PPL: 115.716\n",
            "\t Val. Loss: 4.736 |  Val. PPL: 113.926\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:55<00:00, 31.14it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 153.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 99 | Time: 2m 2s\n",
            "\tTrain Loss: 4.763 | Train PPL: 117.106\n",
            "\t Val. Loss: 4.749 |  Val. PPL: 115.453\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3598/3598 [01:54<00:00, 31.39it/s]\n",
            "100%|██████████| 122/122 [00:00<00:00, 154.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 100 | Time: 1m 58s\n",
            "\tTrain Loss: 4.783 | Train PPL: 119.484\n",
            "\t Val. Loss: 4.753 |  Val. PPL: 115.896\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 100\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_example = []\n",
        "    val_example = []\n",
        "\n",
        "    for i in range(train_df.shape[0]):\n",
        "        try:\n",
        "            ex = data.Example.fromlist([train_df.question[i], train_df.solution[i]], fields)\n",
        "            train_example.append(ex)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    for i in range(val_df.shape[0]):\n",
        "        try:\n",
        "            ex = data.Example.fromlist([val_df.question[i], val_df.solution[i]], fields)\n",
        "            val_example.append(ex)\n",
        "        except:\n",
        "            pass       \n",
        "\n",
        "    train_data = data.Dataset(train_example, fields)\n",
        "    valid_data =  data.Dataset(val_example, fields)\n",
        "\n",
        "    BATCH_SIZE = 1\n",
        "    train_iterator, valid_iterator = BucketIterator.splits((train_data, valid_data), batch_size = BATCH_SIZE, \n",
        "                                                                sort_key = lambda x: len(x.Input),\n",
        "                                                                sort_within_batch=True, device = device)\n",
        "\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), model_saved_path)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MMKkB13yPeJ7"
      },
      "outputs": [],
      "source": [
        "SRC = Input\n",
        "TRG = Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3_aq7QTPBFc"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(model_saved_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieIjql9uPKH1"
      },
      "outputs": [],
      "source": [
        "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50000):\n",
        "    \n",
        "    model.eval()\n",
        "        \n",
        "    if isinstance(sentence, str):\n",
        "        nlp = spacy.load('en')\n",
        "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "        \n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
        "\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "    \n",
        "    src_mask = model.make_src_mask(src_tensor)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\n",
        "\n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
        "\n",
        "    for i in range(max_len):\n",
        "\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "        trg_mask = model.make_trg_mask(trg_tensor)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        pred_token = output.argmax(2)[:,-1].item()\n",
        "        \n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "            break\n",
        "    \n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
        "    \n",
        "    return trg_tokens[1:], attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mlHzigPJpLT"
      },
      "source": [
        "## Displaying Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTBipndfPOlX"
      },
      "outputs": [],
      "source": [
        "def display_attention(sentence, translation, attention, n_heads = 8, n_rows = 4, n_cols = 2):\n",
        "    \n",
        "    assert n_rows * n_cols == n_heads\n",
        "    \n",
        "    fig = plt.figure(figsize=(30,50))\n",
        "    \n",
        "    for i in range(n_heads):\n",
        "        \n",
        "        ax = fig.add_subplot(n_rows, n_cols, i+1)\n",
        "        \n",
        "        _attention = attention.squeeze(0)[i].cpu().detach().numpy()\n",
        "\n",
        "        cax = ax.matshow(_attention, cmap='bone')\n",
        "\n",
        "        ax.tick_params(labelsize=12)\n",
        "        ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], \n",
        "                           rotation=45)\n",
        "        ax.set_yticklabels(['']+translation)\n",
        "\n",
        "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIyQeFx-dMlK",
        "outputId": "92881acf-6146-44d8-da21-1f086e7bb2d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "predicted trg sequence: \n",
            "[(57, 'utf-8'), (1, 'def'), (1, '__init__'), (53, '('), (1, 'var_1'), (53, ')'), (53, ':'), (4, '\\n'), (5, '    '), (1, 'return'), (53, '['), (53, ']'), (4, '\\n'), (56, '\\n'), (6, ''), (1, 'def'), (1, '__init__'), (53, '('), (1, 'self'), (53, ','), (1, 'var_1'), (53, ')'), (53, ':'), (4, '\\n'), (5, '    '), (1, 'return'), (1, 'len'), (53, '('), (1, 'self'), (53, ','), (1, 'var_1'), (53, ')'), (53, ':'), (4, '\\n'), (5, '        '), (1, 'return'), (53, '['), (53, ']'), (4, '\\n'), (56, '\\n'), (6, ''), (1, 'def'), (1, '__init__'), (53, '('), (1, 'self'), (53, ','), (1, 'var_1'), (53, ')'), (53, ':'), (4, '\\n'), (5, '        '), (1, 'if'), (1, 'not'), (1, 'in'), (1, 'range'), (53, '('), (1, 'self'), (53, ')'), (53, ':'), (4, '\\n'), (5, '        '), (1, 'return'), (53, '['), (2, '0'), (53, ']'), (4, '\\n'), (6, ''), (1, 'return'), (53, '['), (2, '0'), (53, ']'), (4, '\\n'), (56, '\\n'), (6, ''), (1, 'return'), (53, '['), (53, ']'), (4, '\\n'), (56, '\\n'), (6, ''), (1, 'def'), (1, '__init__'), (53, '('), (1, 'self'), (53, ','), (1, 'var_1'), (53, ')'), (53, ':'), (4, '\\n'), (5, '    '), (1, 'def'), (1, '__init__'), (53, '('), (1, 'self'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '4'), (53, ']'), (4, '\\n'), (5, '    '), (1, 'if'), (1, 'var_1'), (53, ')'), (53, ']'), (4, '\\n'), (5, '        '), (1, 'if'), (1, 'len'), (53, '('), (1, 'nums'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ']'), (4, '\\n'), (5, '        '), (1, 'if'), (1, 'nums'), (53, ')'), (53, ']'), (4, '\\n'), (5, '        '), (1, 'if'), (1, 'var_1'), (53, ')'), (53, ':'), (4, '\\n'), (5, '        '), (1, 'return'), (1, 'False'), (4, '\\n'), (56, '\\n'), (53, ')'), (53, ':'), (4, '\\n'), (6, ''), (1, 'else'), (53, ':'), (4, '\\n'), (5, '        '), (1, 'if'), (1, 'var_1'), (53, '['), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ']'), (4, '\\n'), (6, ''), (1, 'else'), (53, ':'), (4, '\\n'), (6, ''), (6, ''), (6, ''), (1, 'else'), (53, ':'), (4, '\\n'), (5, '        '), (1, 'if'), (1, 'nums'), (53, '['), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ','), (2, '1'), (4, '\\n'), (5, '        '), (1, 'if'), (1, 'nums'), (53, '='), (53, '['), (2, '1'), (53, ','), (2, '0'), (53, ','), (2, '0'), (53, ','), (2, '1'), (53, ','), (2, '0'), (53, ']'), (4, '\\n'), (5, '        '), (1, 'return'), (1, 'False'), (4, '\\n'), (56, '\\n'), (53, ')'), (53, ':'), (4, '\\n'), (56, '\\n'), (53, ')'), (53, ':'), (4, '\\n'), (56, '\\n'), (6, ''), (1, 'else'), (53, ':'), (4, '\\n'), (56, '\\n'), (53, ']'), (4, '\\n'), (5, '        '), (1, 'return'), (1, 'False'), (4, '\\n'), (56, '\\n'), (56, '\\n'), (53, ']'), (4, '\\n'), (56, '\\n'), (6, ''), (1, 'else'), (53, ':'), (4, '\\n'), (56, '\\n'), (2, '1'), (4, '\\n'), (5, '        '), (1, 'if'), (1, '__name__'), (53, '=='), (3, '\"__main__\"'), (53, ':'), (4, '\\n'), (56, '\\n'), (6, ''), (1, 'else'), (53, ':'), (4, '\\n'), (56, '\\n'), (53, ')'), (53, ':'), (4, '\\n'), (56, '\\n'), (2, '1'), (4, '\\n'), (56, '\\n'), (53, ')'), (53, ':'), (4, '\\n'), (5, '        '), (1, 'if'), (1, '__name__'), (53, '=='), (3, '\"__main__\"'), (53, ':'), (4, '\\n'), (56, '\\n'), (6, ''), (6, ''), (6, ''), (6, ''), (6, ''), (6, ''), (6, ''), (6, ''), (6, ''), (6, ''), (6, ''), (6, ''), (6, ''), (6, ''), (6, ''), (6, ''), (6, ''), (6, ''), (6, ''), (6, ''), (6, ''), (6, ''), (6, ''), (6, ''), (6, ''), (6, ''), (6, ''), (6, ''), (6, ''), (6, ''), (6, ''), (6, ''), (0, ''), '<eos>']\n"
          ]
        }
      ],
      "source": [
        "src = \"write a function that adds two numbers\"\n",
        "src=src.split(\" \")\n",
        "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
        "\n",
        "print(f'predicted trg sequence: ')\n",
        "print(translation)\n",
        "# print(\"code: \\n\", untokenize(translation[:-1]).decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTcWk_4x71ty",
        "outputId": "1416ca4f-5a3d-4ae8-dd1d-207922af4c58"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torchtext.legacy.data.example.Example at 0x7f7f3abb8e50>"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "valid_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHWqhmvtPTJv"
      },
      "outputs": [],
      "source": [
        "display_attention(src, translation, attention)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}